# -*- coding: utf-8 -*-
"""data_mining_finetuninng.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gmbvzZePbVF-QsbtWIw7uXIQlHe6ovi8
"""

import argparse
import os
import sys
# from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel, AutoConfig
import wandb
import numpy as np

config = AutoConfig.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")



# Commented out IPython magic to ensure Python compatibility.
# %%writefile ex1.py
# import argparse
# import sys
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from datasets import Dataset
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
# from sklearn.metrics import precision_recall_fscore_support, accuracy_score
# import wandb
# import torch
# import logging
# 
# # Set up logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# 
# def parse_args():
#     parser = argparse.ArgumentParser(description="Fine-tune BERT for paper category classification")
#     parser.add_argument("--max_train_samples", type=int, default=-1, help="Number of training samples or -1 for all")
#     parser.add_argument("--max_eval_samples", type=int, default=-1, help="Number of validation samples or -1 for all")
#     parser.add_argument("--max_predict_samples", type=int, default=-1, help="Number of test samples or -1 for all")
#     parser.add_argument("--num_train_epochs", type=int, default=3, help="Number of training epochs")
#     parser.add_argument("--lr", type=float, default=2e-5, help="Learning rate")
#     parser.add_argument("--batch_size", type=int, default=16, help="Batch size for training and evaluation")
#     parser.add_argument("--do_train", action="store_true", help="Run training")
#     parser.add_argument("--do_predict", action="store_true", help="Run prediction")
#     parser.add_argument("--model_path", type=str, default="./results", help="Path to save/load model")
#     return parser.parse_args()
# 
# def load_and_prepare_data(args, df):
#     # Clean and preprocess DataFrame
#     df = df.dropna(subset=['abstract', 'categories'])
#     df['categories'] = df['categories'].str.strip()
#     logger.info(f"Loaded dataset with {len(df)} samples after removing NaNs")
#     logger.info(f"Categories distribution:\n{df['categories'].value_counts().to_string()}")
# 
#     if len(df) < 100:
#         logger.warning("Dataset is small (<100 samples). Consider adding more samples for effective fine-tuning.")
# 
#     # Encode categories
#     label_encoder = LabelEncoder()
#     df['label'] = label_encoder.fit_transform(df['categories'])
#     num_labels = len(label_encoder.classes_)
#     logger.info(f"Number of labels: {num_labels}")
#     logger.info(f"Label mapping: {dict(zip(label_encoder.classes_, range(num_labels)))}")
# 
#     # Split data into train (20%), validation (40%), test (40%)
#     train_df, temp_df = train_test_split(df, test_size=0.8, random_state=42, stratify=df['label'])
#     eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])
# 
#     logger.info(f"Train size: {len(train_df)}, Eval size: {len(eval_df)}, Test size: {len(test_df)}")
# 
#     # Reset indices
#     train_df = train_df.reset_index(drop=True)
#     eval_df = eval_df.reset_index(drop=True)
#     test_df = test_df.reset_index(drop=True)
# 
#     # Convert to Hugging Face Datasets
#     train_dataset = Dataset.from_pandas(train_df[['abstract', 'label']])
#     eval_dataset = Dataset.from_pandas(eval_df[['abstract', 'label']])
#     test_dataset = Dataset.from_pandas(test_df[['abstract', 'label']])
# 
#     # Load tokenizer
#     tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
# 
#     # Tokenization function
#     def tokenize_function(examples):
#         return tokenizer(
#             [str(abstract) for abstract in examples["abstract"]],
#             padding="max_length",
#             truncation=True,
#             max_length=512
#         )
# 
#     # Tokenize datasets
#     train_dataset = train_dataset.map(tokenize_function, batched=True)
#     eval_dataset = eval_dataset.map(tokenize_function, batched=True)
#     test_dataset = test_dataset.map(tokenize_function, batched=True)
# 
#     # Subset data based on args
#     if args.max_train_samples != -1:
#         train_dataset = train_dataset.select(range(min(args.max_train_samples, len(train_dataset))))
#     if args.max_eval_samples != -1:
#         eval_dataset = eval_dataset.select(range(min(args.max_eval_samples, len(eval_dataset))))
#     if args.max_predict_samples != -1:
#         test_dataset = test_dataset.select(range(min(args.max_predict_samples, len(test_dataset))))
# 
#     # Set format for PyTorch
#     train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
#     eval_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
#     test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
# 
#     return train_dataset, eval_dataset, test_dataset, tokenizer, label_encoder, num_labels
# 
# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     predictions = np.argmax(predictions, axis=1)
#     accuracy = accuracy_score(labels, predictions)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions)
#     return {
#         "accuracy": accuracy,
#         "precision": precision,
#         "recall": recall,
#         "f1": f1
#     }
# 
# def train_model(args, train_dataset, eval_dataset, num_labels):
#     # Load model
#     model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
# 
#     # Define training arguments
#     training_args = TrainingArguments(
#         output_dir=args.model_path,
#         eval_strategy="epoch",  # Changed from evaluation_strategy
#         save_strategy="epoch",
#         learning_rate=args.lr,
#         per_device_train_batch_size=args.batch_size,
#         per_device_eval_batch_size=args.batch_size,
#         num_train_epochs=args.num_train_epochs,
#         weight_decay=0.01,
#         logging_steps=10,
#         save_total_limit=2,
#         report_to="wandb",
#         run_name=f"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}"
#     )
# 
#     # Initialize trainer
#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         train_dataset=train_dataset,
#         eval_dataset=eval_dataset,
#         compute_metrics=compute_metrics
#     )
# 
#     # Train
#     trainer.train()
# 
#     # Evaluate and save results
#     eval_results = trainer.evaluate()
#     logger.info(f"Evaluation results: {eval_results}")
#     with open("res.txt", "a") as f:
#         f.write(f"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}: {eval_results}\n")
# 
#     # Save model
#     trainer.save_model(args.model_path)
#     return trainer
# 
# def predict(trainer, test_dataset, label_encoder):
#     # Make predictions
#     predictions = trainer.predict(test_dataset)
#     preds = predictions.predictions.argmax(-1)
# 
#     # Convert numerical predictions to category names
#     pred_categories = label_encoder.inverse_transform(preds)
# 
#     # Save predictions
#     with open("predictions.txt", "w") as f:
#         for pred in pred_categories:
#             f.write(f"{pred}\n")
# 
# def main(df):
#     args = parse_args()
# 
#     # Initialize Weights & Biases (optional)
#     try:
#         wandb.login()
#     except Exception as e:
#         logger.warning(f"Failed to login to W&B: {e}. Continuing without W&B logging.")
#         args.report_to = None
# 
#     # Load and prepare data
#     train_dataset, eval_dataset, test_dataset, tokenizer, label_encoder, num_labels = load_and_prepare_data(args, df)
# 
#     if args.do_train:
#         trainer = train_model(args, train_dataset, eval_dataset, num_labels)
# 
#     if args.do_predict:
#         if not args.do_train:
#             model = AutoModelForSequenceClassification.from_pretrained(args.model_path, num_labels=num_labels)
#             training_args = TrainingArguments(
#                 output_dir=args.model_path,
#                 per_device_eval_batch_size=args.batch_size,
#             )
#             trainer = Trainer(
#                 model=model,
#                 args=training_args,
#                 compute_metrics=compute_metrics
#             )
#         predict(trainer, test_dataset, label_encoder)
# 
# if __name__ == "__main__":
#     try:
#         # Load DataFrame with error handling
#         df = pd.read_csv('one_category.csv', sep=',', on_bad_lines='skip', engine='python')
#         logger.info(f"Raw CSV loaded with {len(df)} rows")
#         data = df[df['categories'].isin({'hep-ph', 'math.NT', 'cond-mat.soft', 'cs.DS', 'q-bio.OT', 'nucl-th'})]
#         data = data[['abstract', 'categories']]
#         if data.empty:
#             raise ValueError("Filtered dataset is empty. Check category filtering or input data.")
#         logger.info(f"Filtered dataset with {len(data)} rows")
#         main(data)
#     except Exception as e:
#         logger.error(f"Error in main: {e}")
#         raise

!python ex1.py --max_train_samples -1 --max_eval_samples -1 --num_train_epochs 1 --lr 2e-5 --batch_size 16 --do_train

from google.colab import drive
drive.mount('/content/drive')

!cp -r ./results /content/drive/MyDrive/bert_model_results

'''
for later use
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/bert_model_results')
'''

pip install numpy==1.26.4

df['abstract'][0]

import argparse
import sys
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import wandb
import torch
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune BERT for paper category classification")
    parser.add_argument("--max_train_samples", type=int, default=-1, help="Number of training samples or -1 for all")
    parser.add_argument("--max_eval_samples", type=int, default=-1, help="Number of validation samples or -1 for all")
    parser.add_argument("--max_predict_samples", type=int, default=-1, help="Number of test samples or -1 for all")
    parser.add_argument("--num_train_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--lr", type=float, default=2e-5, help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for training and evaluation")
    parser.add_argument("--do_train", action="store_true", help="Run training")
    parser.add_argument("--do_predict", action="store_true", help="Run prediction")
    parser.add_argument("--model_path", type=str, default="./results", help="Path to save/load model")
    return parser.parse_args()

def load_and_prepare_data(args, df):
    # Clean and preprocess DataFrame
    df = df.dropna(subset=['abstract', 'categories'])
    df['categories'] = df['categories'].str.strip()
    logger.info(f"Loaded dataset with {len(df)} samples after removing NaNs")
    logger.info(f"Categories distribution:\n{df['categories'].value_counts().to_string()}")

    if len(df) < 100:
        logger.warning("Dataset is small (<100 samples). Consider adding more samples for effective fine-tuning.")

    # Encode categories
    label_encoder = LabelEncoder()
    df['label'] = label_encoder.fit_transform(df['categories'])
    num_labels = len(label_encoder.classes_)
    logger.info(f"Number of labels: {num_labels}")
    logger.info(f"Label mapping: {dict(zip(label_encoder.classes_, range(num_labels)))}")

    # Split data into train (20%), validation (40%), test (40%)
    train_df, temp_df = train_test_split(df, test_size=0.8, random_state=42, stratify=df['label'])
    eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

    logger.info(f"Train size: {len(train_df)}, Eval size: {len(eval_df)}, Test size: {len(test_df)}")

    # Reset indices
    train_df = train_df.reset_index(drop=True)
    eval_df = eval_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    # Convert to Hugging Face Datasets
    train_dataset = Dataset.from_pandas(train_df[['abstract', 'label']])
    eval_dataset = Dataset.from_pandas(eval_df[['abstract', 'label']])
    test_dataset = Dataset.from_pandas(test_df[['abstract', 'label']])

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    # Tokenization function
    def tokenize_function(examples):
        return tokenizer(
            [str(abstract) for abstract in examples["abstract"]],
            padding="max_length",
            truncation=True,
            max_length=512
        )

    # Tokenize datasets
    train_dataset = train_dataset.map(tokenize_function, batched=True)
    eval_dataset = eval_dataset.map(tokenize_function, batched=True)
    test_dataset = test_dataset.map(tokenize_function, batched=True)

    # Subset data based on args
    if args.max_train_samples != -1:
        train_dataset = train_dataset.select(range(min(args.max_train_samples, len(train_dataset))))
    if args.max_eval_samples != -1:
        eval_dataset = eval_dataset.select(range(min(args.max_eval_samples, len(eval_dataset))))
    if args.max_predict_samples != -1:
        test_dataset = test_dataset.select(range(min(args.max_predict_samples, len(test_dataset))))

    # Set format for PyTorch
    train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    eval_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

    return train_dataset, eval_dataset, test_dataset, tokenizer, label_encoder, num_labels

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions)
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

def train_model(args, train_dataset, eval_dataset, num_labels):
    # Load model
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)

    # Define training arguments
    training_args = TrainingArguments(
        output_dir=args.model_path,
        eval_strategy="epoch",  # Changed from evaluation_strategy
        save_strategy="epoch",
        learning_rate=args.lr,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        num_train_epochs=args.num_train_epochs,
        weight_decay=0.01,
        logging_steps=10,
        save_total_limit=2,
        report_to="wandb",
        run_name=f"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}"
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics
    )

    # Train
    trainer.train()

    # Evaluate and save results
    eval_results = trainer.evaluate()
    logger.info(f"Evaluation results: {eval_results}")
    with open("res.txt", "a") as f:
        f.write(f"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}: {eval_results}\n")

    # Save model
    trainer.save_model(args.model_path)
    return trainer

def predict(trainer, test_dataset, label_encoder):
    # Make predictions
    predictions = trainer.predict(test_dataset)
    preds = predictions.predictions.argmax(-1)

    # Convert numerical predictions to category names
    pred_categories = label_encoder.inverse_transform(preds)

    # Save predictions
    with open("predictions.txt", "w") as f:
        for pred in pred_categories:
            f.write(f"{pred}\n")

def main(df):
    args = parse_args()

    # Initialize Weights & Biases (optional)
    try:
        wandb.login()
    except Exception as e:
        logger.warning(f"Failed to login to W&B: {e}. Continuing without W&B logging.")
        args.report_to = None

    # Load and prepare data
    train_dataset, eval_dataset, test_dataset, tokenizer, label_encoder, num_labels = load_and_prepare_data(args, df)

    if args.do_train:
        trainer = train_model(args, train_dataset, eval_dataset, num_labels)

    if args.do_predict:
        if not args.do_train:
            model = AutoModelForSequenceClassification.from_pretrained(args.model_path, num_labels=num_labels)
            training_args = TrainingArguments(
                output_dir=args.model_path,
                per_device_eval_batch_size=args.batch_size,
            )
            trainer = Trainer(
                model=model,
                args=training_args,
                compute_metrics=compute_metrics
            )
        predict(trainer, test_dataset, label_encoder)

if __name__ == "__main__":
    try:
        # Load DataFrame with error handling
        df = pd.read_csv('one_category.csv', sep=',', on_bad_lines='skip', engine='python')
        logger.info(f"Raw CSV loaded with {len(df)} rows")
        data = df[df['categories'].isin({'hep-ph', 'math.NT', 'cond-mat.soft', 'cs.DS', 'q-bio.OT', 'nucl-th'})]
        data = data[['abstract', 'categories']]
        if data.empty:
            raise ValueError("Filtered dataset is empty. Check category filtering or input data.")
        logger.info(f"Filtered dataset with {len(data)} rows")
        main(data)
    except Exception as e:
        logger.error(f"Error in main: {e}")
        raise

import pandas as pd
df = pd.read_csv('one_category.csv', sep=',', on_bad_lines='skip', engine='python')
logger.info(f"Raw CSV loaded with {len(df)} rows")
df = df[df['categories'].isin({'hep-ph', 'math.NT', 'cond-mat.soft', 'cs.DS', 'q-bio.OT', 'nucl-th'})]
df = df[['abstract', 'categories']]

df = df.dropna(subset=['abstract', 'categories'])
df['categories'] = df['categories'].str.strip()
logger.info(f"Loaded dataset with {len(df)} samples after removing NaNs")
logger.info(f"Categories distribution:\n{df['categories'].value_counts().to_string()}")

if len(df) < 100:
    logger.warning("Dataset is small (<100 samples). Consider adding more samples for effective fine-tuning.")

# Encode categories
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['categories'])
num_labels = len(label_encoder.classes_)
logger.info(f"Number of labels: {num_labels}")
logger.info(f"Label mapping: {dict(zip(label_encoder.classes_, range(num_labels)))}")

# Split data into train (20%), validation (40%), test (40%)
train_df, temp_df = train_test_split(df, test_size=0.8, random_state=42, stratify=df['label'])
eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

logger.info(f"Train size: {len(train_df)}, Eval size: {len(eval_df)}, Test size: {len(test_df)}")

# Reset indices
train_df = train_df.reset_index(drop=True)
eval_df = eval_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

# Convert to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df[['abstract', 'label']])
eval_dataset = Dataset.from_pandas(eval_df[['abstract', 'label']])
test_dataset = Dataset.from_pandas(test_df[['abstract', 'label']])

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        [str(abstract) for abstract in examples["abstract"]],
        padding="max_length",
        truncation=True,
        max_length=512
    )

# Tokenize datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

test_dataset

"""Example use for infrence

"""

example_abstract = '''A fully differential calculation in perturbative quantum chromodynamics is
presented for the production of massive photon pairs at hadron colliders. All
next-to-leading order perturbative contributions from quark-antiquark,
gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as
all-orders resummation of initial-state gluon radiation valid at
next-to-next-to-leading logarithmic accuracy. The region of phase space is
specified in which the calculation is most reliable. Good agreement is
demonstrated with data from the Fermilab Tevatron, and predictions are made for
more detailed tests with CDF and DO data. Predictions are shown for
distributions of diphoton pairs produced at the energy of the Large Hadron
Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs
boson are contrasted with those produced from QCD processes at the LHC, showing
that enhanced sensitivity to the signal can be obtained with judicious
selection of events.'''

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import logging
import numpy as np

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define paths and categories
model_path = '/content/drive/MyDrive/bert_model_results'
categories = ['hep-ph', 'math.NT', 'cond-mat.soft', 'cs.DS', 'q-bio.OT', 'nucl-th']

# Verify model directory exists
import os
if not os.path.exists(model_path):
    logger.error(f"Model path {model_path} does not exist. Check if the model was saved correctly.")
    raise FileNotFoundError(f"Model path {model_path} not found")

# Load tokenizer

# Load model
try:
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    logger.info(f"Model loaded from {model_path}")
except Exception as e:
    logger.error(f"Error loading model from {model_path}: {e}")
    raise

# Recreate label encoder
label_encoder = LabelEncoder()
label_encoder.fit(categories)
logger.info(f"Label encoder created with categories: {categories}")

# Set device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)
model.eval()
logger.info(f"Model set to evaluation mode on {device}")

# Tokenize the abstract
inputs = tokenizer(
    example_abstract,
    padding="max_length",
    truncation=True,
    max_length=512,
    return_tensors="pt"
)
logger.info("Abstract tokenized")

# Move inputs to device
inputs = {key: val.to(device) for key, val in inputs.items()}

# Make prediction
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]
logger.info("Prediction made")

# Convert to category name
predicted_category = label_encoder.inverse_transform([predicted_label])[0]

# Log and save result
logger.info(f"Abstract: {example_abstract[:100]}...")
logger.info(f"Predicted category: {predicted_category}")
with open('/content/inference_result.txt', 'w') as f:
    f.write(f"Abstract: {example_abstract[:100]}...\nPredicted category: {predicted_category}\n")
logger.info("Result saved to /content/inference_result.txt")

# Copy result to Google Drive for persistence
try:
    os.makedirs('/content/drive/MyDrive/bert_model_results/outputs', exist_ok=True)
    os.system('cp /content/inference_result.txt /content/drive/MyDrive/bert_model_results/outputs/inference_result.txt')
    logger.info("Result copied to /content/drive/MyDrive/bert_model_results/outputs/inference_result.txt")
except Exception as e:
    logger.warning(f"Failed to copy result to Google Drive: {e}")

predicted_category

# Create output directory
output_dir = '/content/drive/MyDrive/bert_model_results/outputs'
os.makedirs(output_dir, exist_ok=True)
logger.info(f"Output directory created at {output_dir}")

# Recreate label encoder
label_encoder = LabelEncoder()
label_encoder.fit(categories)
logger.info(f"Label encoder created with categories: {categories}")

# Define metrics function (per-class scores)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None)
    return {
        "accuracy": accuracy,
        "precision_per_class": precision.tolist(),
        "recall_per_class": recall.tolist(),
        "f1_per_class": f1.tolist()
    }

# Verify test dataset
logger.info(f"Test dataset info: {test_dataset}")

# Set device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)
logger.info(f"Model moved to {device}")

# Define training arguments for evaluation
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_eval_batch_size=16,
    report_to="none",
    do_train=False,
    do_eval=True
)
logger.info("Training arguments defined")

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    eval_dataset=test_dataset
)
logger.info("Trainer initialized")

# Run evaluation
try:
    eval_results = trainer.evaluate()
    logger.info(f"Evaluation results: {eval_results}")
except Exception as e:
    logger.error(f"Error during evaluation: {e}")
    raise

# Format per-class metrics with category names
metrics_with_categories = {
    "accuracy": eval_results["eval_accuracy"],
    "eval_loss": eval_results["eval_loss"],
    "precision_per_class": {cat: prec for cat, prec in zip(categories, eval_results["eval_precision_per_class"])},
    "recall_per_class": {cat: rec for cat, rec in zip(categories, eval_results["eval_recall_per_class"])},
    "f1_per_class": {cat: f1 for cat, f1 in zip(categories, eval_results["eval_f1_per_class"])}
}

import json
# Save evaluation results
with open(os.path.join(output_dir, 'test_eval_results_per_class.txt'), 'w') as f:
    f.write(json.dumps(metrics_with_categories, indent=2))
logger.info(f"Evaluation results saved to {os.path.join(output_dir, 'test_eval_results_per_class.txt')}")

# Make predictions (for detailed analysis)
try:
    predictions = trainer.predict(test_dataset)
    pred_logits = predictions.predictions
    pred_labels = np.argmax(pred_logits, axis=1)
    true_labels = predictions.label_ids
    pred_categories = label_encoder.inverse_transform(pred_labels)
    true_categories = label_encoder.inverse_transform(true_labels)
    logger.info("Predictions made")
except Exception as e:
    logger.error(f"Error during prediction: {e}")
    raise

# Save predictions to a file
with open(os.path.join(output_dir, 'test_eval_predictions_per_class.txt'), 'w') as f:
    for abstract, pred, true in zip(test_dataset['abstract'], pred_categories, true_categories):
        f.write(f"Abstract: {abstract[:100]}...\nPredicted: {pred}, True: {true}\n\n")
logger.info(f"Predictions saved to {os.path.join(output_dir, 'test_eval_predictions_per_class.txt')}")



import pandas as pd
df = pd.read_csv('one_category.csv', sep=',', on_bad_lines='skip', engine='python')

data = df[['abstract', 'categories']]

top_n = 20
data['categories'].value_counts().nlargest(top_n).plot(kind='bar', figsize=(12, 6))
plt.title(f'Top {top_n} Categories')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

data = data[data['categories'].isin({'hep-ph', 'math.NT', 'cond-mat.soft', 'cs.DS', 'q-bio.OT', 'nucl-th'})]
top_n = 20
data['categories'].value_counts().nlargest(top_n).plot(kind='bar', figsize=(12, 6))
plt.title(f'chosen Categories')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

